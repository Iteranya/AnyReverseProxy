# ğŸŒ€ OpenAI Compatible Reverse Proxy

Exactly what it says on the tin:
Plug in your API key, set the endpoint, choose your port â€” fire it up â€” and *presto!* Youâ€™ve got your very own AI Endpoint, no fancy wallet required.

## âš™ï¸ How It Works

1. Drop your `API_KEY`, `API_ENDPOINT`, and `PORT` in a `.env` file.
2. Run the thing.
3. Point your apps at it like itâ€™s OpenAI â€” because it *is* (sort of).

## âœ… What Works

* **Sequential requests** â€” like a polite line at the post office.
* Perfect for cheapskates (hello, yes, itâ€™s me) who just want their LLM fix without paying for fancy concurrency.

## ğŸ§ª Coming Soonâ„¢

* ğŸ”‘ **Multi-Key Support** â€” so you donâ€™t get rate-limited into oblivion.
* ğŸ”— **Multi-Provider Support** â€” run OpenRouter, local models, or your neighborâ€™s AI rig.
* âš¡ **Parallel Processing** â€” so your requests can run like caffeinated squirrels instead of a single sloth.

## ğŸš€ Quickstart

```bash
# 1ï¸ Clone it
git https://github.com/Iteranya/AnyReverseProxy.git
cd AnyReverseProxy

# 2ï¸ Make your environment
python -m venv/venv
venv/scripts/activate (in windows)
source venv/bin/activate (in linux)
cp .env.example .env

# Example .env
# --------------------------
# API_ENDPOINT=https://openrouter.ai/api/v1
# API_KEY=MLEM
# PORT=5000
# --------------------------

# 3ï¸ Install dependencies
pip install -r requirements.txt

# 4ï¸ Run it
python main.py
```

Now hit `http://localhost:PORT` with your OpenAI-compatible requests. Models, completions, chat â€” all yours, baby.

## ğŸ“Œ Notes

* This proxy just forwards your requests â€” the actual magic still happens at the real endpoint.
* Keep your API key safe. Or donâ€™t. Iâ€™m not your mom.
* Contributions, feature ideas, or bribes welcome.

---

### âš¡ License

AGPL-3, Like All Of My Project